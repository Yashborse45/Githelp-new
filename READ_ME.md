# RePoMIND - Streamlit (converted)

This project was auto-generated from your uploaded notebook and adapted to use Ollama (local) for LLM calls.

Run with:

```
python -m venv .venv
source .venv/bin/activate  # or Windows equivalent
pip install -r requirements.txt
streamlit run app.py
```

Make sure `ollama serve` is running and model is pulled: `ollama pull llama3` (or your model).